# The Abstraction Scaling Test
Our brains are great at using analogy and comparison to make thinking about things less expensive. So much so that we 
do this automatically, just like in inversions. This leads to biases like assuming things having human qualities 
(anthropomorphization), false analogies, and category errors. This happens all the time, and we can't stop it, so
we need a way to test our reasoning to make sure that our analogies aren't reflexive like oversimplified dualism is.

For this, I use an 'Abstraction Scaling Test.' If realization creates details and generalization smooths them over, we
should consider the range of scales over which we're being asked to apply an analogy. It makes sense to be cautious
when asked to use an analogy across the scale of 'individual human being' to 'the observable Universe.' Or even
'individual human being' to 'the cells in that being's brain.' We'll have thrown away so much information to get there 
that we should question if the conclusion has any meaning at all.

My favorite example is 'consciousness' and how far people stretch it. A concept has a kind of 'home' on the scale
of abstraction where it's most clearly defined. For 'consciousness,' that's the 'individual person' level because
consciousness is experienced at that level. 
Now let's consider a shovel's 'consciousness.' To what degree does a shovel display 'consciousness' or 'meaning to 
consciousness?' Well, it has none of its own, but from it, we can catch hints about the consciousness
that made it. A shovel has parts—a handle, a curved, flat blade—that implies that the mind that created it thinks in 
parts. Something's context has a huge impact on what we infer from it. A shovel in the trunk of a ditchdigger is normal.
A shovel in the trunk of a serial killer raises reasonable concerns. 

In this situation, we're abstracting a *short distance* from the concept's home. It's easy to think of someone holding
a shovel, using a shovel. There's a natural border between the 'individual' and the shovel that they share, usually 
the hands or, if someone is tragically misinformed, the teeth.

Let's contrast this with, say, neurons in the brain. They're arguably 'closest' to consciousness in the sense that 
they're right there as it's happening. However, trying to analogize between consciousness and neurons throws away so much 
information in the process that we need to consider how reasonable that move is. Neurons exist in an environment that,
while it is US, is NOT the one in which we exist. They are cells, not individual people. The analogy crosses a distance
that is *too long* to preserve meaning in any but the most broad sense of 'is it alive?'

Now consider machine learning and AI these days. 'Consciousness' is thrown around in the debates, but I think that's
an abstraction that crosses too many boundaries to preserve meaning. We are asked to analogize from ourselves to
computers, asking 'can it become like us, or superior to us?' We project human desires onto hypothetical AI 
consciousness. We get advertisements saying 'it's like having a PhD-level assistant in everything!' Really? Because
mine once invented a Java library that doesn't exist, complete with a tutorial, to solve a problem I asked about. If 
that is PhD, we should consider toddlers to deserve diplomas for mastering object permanence.

The problem here is that the analogy of 'they sound like us, they must think how we think' is an abstraction chain that
is incredibly, uselessly long and complicated. The nuances of machine learning and transformer architecture doesn't sell
Grok Pro subscriptions, so they're shoved aside for a hilariously overbroad analogy that makes that money. This case
fails the Abstraction Scaling Test, and so should be considered suspect, at best. 